{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "OGUZRFu0YmVY",
        "outputId": "1e0be6c0-2cc9-48cf-8a76-07f50a0e5fba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-17fb9169bf35>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mytextfile.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mytextfile.txt'"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Load data\n",
        "with open('mytextfile.txt', 'r', encoding='ISO-8859-1') as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(token)\n",
        "    return result\n",
        "\n",
        "processed_data = [preprocess(doc) for doc in data]\n",
        "\n",
        "# Create a dictionary from the processed data\n",
        "dictionary = corpora.Dictionary(processed_data)\n",
        "\n",
        "# Create a document-term matrix\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_data]\n",
        "\n",
        "# Train the LDA model\n",
        "num_topics = 10\n",
        "lda_model = LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "# Interpret the topics\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f'Topic: {idx} \\nWords: {topic}\\n')\n",
        "\n",
        "# Compute the coherence score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_data, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the topics\n",
        "import pyLDAvis.gensim\n",
        "lda_display = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\n",
        "pyLDAvis.display(lda_display)\n",
        "\n",
        "lda_display = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "U0BHlrBWYnsk",
        "outputId": "cabe6e37-c199-432f-cdc1-5247c78896f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-61032e2d1f76>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlda_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import scipy.cluster.hierarchy as shc\n",
        "\n",
        "# # Create a list of high-frequency words\n",
        "# freq_words = [token for doc in processed_data for token in doc if dictionary.doc2bow([token])[0][1] > 50]\n",
        "\n",
        "# # Sort the dictionary by frequency of occurrence\n",
        "# dict_sorted = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# # Get the top 100 high-frequency words from the sorted dictionary\n",
        "# top_words = [word for word, freq in dict_sorted[:10]]\n",
        "\n",
        "# # Create a document-term matrix for high-frequency words\n",
        "# doc_term_matrix_freq = [[dictionary.doc2bow([token])[0][0] for token in doc if token in freq_words] for doc in processed_data]\n",
        "\n",
        "# # Convert the document-term matrix to a numpy array\n",
        "# doc_term_matrix_freq_np = np.zeros((len(doc_term_matrix_freq), len(freq_words)))\n",
        "# for i, doc in enumerate(doc_term_matrix_freq):\n",
        "#     for j, freq_word in enumerate(freq_words):\n",
        "#         if freq_word in doc:\n",
        "#             doc_term_matrix_freq_np[i, j] = 1\n",
        "\n",
        "# # Compute the linkage matrix\n",
        "# Z = shc.linkage(doc_term_matrix_freq_np, method='ward', optimal_ordering=True)\n",
        "\n",
        "# # Create a dendrogram of linkages\n",
        "# fig, ax = plt.subplots(figsize=(8, 6))\n",
        "# plt.title(\"Dendrogram of Linkages for High-Frequency Words\")\n",
        "# dend = shc.dendrogram(Z, labels=[f\"Doc {i}\" for i in range(len(doc_term_matrix_freq))], orientation='right', leaf_font_size=10)\n",
        "# plt.xlabel(\"Distance\")\n",
        "# plt.ylabel(\"Document\")\n",
        "# plt.savefig(\"dendrogram.png\", bbox_inches='tight')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "hu8NKEY0ZTbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Get the word frequencies\n",
        "word_frequencies = {}\n",
        "for doc in processed_data:\n",
        "    for word in doc:\n",
        "        if word in word_frequencies:\n",
        "            word_frequencies[word] += 1\n",
        "        else:\n",
        "            word_frequencies[word] = 1\n",
        "\n",
        "# Get the top 50 most frequent words\n",
        "top_words = [word for word, frequency in sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)[:50]]\n",
        "\n",
        "# Create a document-term matrix for the top words\n",
        "top_doc_term_matrix = [[doc.count(word) for word in top_words] for doc in processed_data]\n",
        "df = pd.DataFrame(top_doc_term_matrix, columns=top_words)\n",
        "\n",
        "# Create the heatmap\n",
        "sns.heatmap(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "asvxq3tuZXNN",
        "outputId": "36f07a1f-a998-4889-e4fc-cea54342b9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1072686d48c3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get the word frequencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_frequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_frequencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'processed_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzCZ1XEkZaSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}